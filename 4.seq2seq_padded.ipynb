{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LEARNING_RATE = 0.01\n",
    "LSTM_HIDDEN_SIZE = 100\n",
    "EMBEDDINGS_DIMS = 50\n",
    "TEACHER_FORCING_PROB = 0.5\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "\n",
    "conversation_pair = [\n",
    "    ['Hi how are you?', 'I am good ,thank you.'],\n",
    "    ['How was your day?', 'It was a fantastic day'],\n",
    "    ['Good morning!', 'Good morning to you too'],\n",
    "    ['How everything is going', 'Things are going great'],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"A simple decoder with word embeddings\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embeddings_dims):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.emb = nn.Embedding(num_embeddings=input_size, embedding_dim=embeddings_dims)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_dims, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        x = self.emb(x)\n",
    "        out, hidden_states = self.lstm(x, hidden_states)\n",
    "        return out, hidden_states\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"A simple decoder with embedding, and a linear layer to project the output of\n",
    "       the layer LSTM to a vocabulary size dimension:  hidden_size -> vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, embeddings_dims, vocab_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=input_size, embedding_dim=embeddings_dims)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_dims, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden_states):\n",
    "        x = self.emb(x)\n",
    "        out, hidden_states = self.lstm(x, hidden_states)\n",
    "        out = self.linear(out)\n",
    "        return out, hidden_states\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Converts Text into its numerical representation\"\"\"\n",
    "\n",
    "    def __init__(self, max_sequence_length):\n",
    "        self.START_TOKEN = '<sos>'\n",
    "        self.PADDING_TOKEN = '<pad>'\n",
    "        self.END_TOKEN = '<eos>'\n",
    "        self.word2index = {self.PADDING_TOKEN: 0, self.START_TOKEN: 1, self.END_TOKEN: 2}\n",
    "        self.index2word = {0: self.PADDING_TOKEN, 1: self.START_TOKEN, 2: self.END_TOKEN}\n",
    "        self.words_count = len(self.word2index)\n",
    "        self.max_length = max_sequence_length\n",
    "\n",
    "    def fit_on_text(self, text_array):\n",
    "        \"\"\"Creates a numerical index value for every unique word\"\"\"\n",
    "        for sentence in text_array:\n",
    "            self._add_sentence(sentence)\n",
    "\n",
    "    def _add_sentence(self, sentence):\n",
    "        \"\"\"Creates indexes for unique word in the sentences and\n",
    "        adds them to the dictionary\"\"\"\n",
    "        for word in sentence.strip().lower().split(' '):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.words_count\n",
    "                self.index2word[self.words_count] = word\n",
    "                self.words_count += 1\n",
    "\n",
    "    def texts_to_index(self, sentences):\n",
    "        \"\"\"Convert words in sentences to their numerical index values\"\"\"\n",
    "        sentences_index = []\n",
    "        end_token_index = self.word2index[self.END_TOKEN]\n",
    "        for sentence in sentences:\n",
    "            sentence_index = []\n",
    "            for word in sentence.strip().lower().split(' '):\n",
    "                sentence_index.append(self.word2index[word])\n",
    "            sentence_index.append(end_token_index)\n",
    "            sentence_index = self._pad(sentence_index)\n",
    "            sentence_index = self._clip(sentence_index)\n",
    "            sentences_index.append(sentence_index)\n",
    "        return sentences_index\n",
    "\n",
    "    def _clip(self, sequence):\n",
    "        return sequence[:self.max_length]\n",
    "\n",
    "    def _pad(self, sequence):\n",
    "        pad_index = self.word2index[self.PADDING_TOKEN]\n",
    "        while len(sequence) < self.max_length:\n",
    "            sequence.append(pad_index)\n",
    "        return sequence\n",
    "\n",
    "    def indexes_to_text(self, word_numbers):\n",
    "        \"\"\"Converts an array of numbers to a text string\"\"\"\n",
    "        ignore_index = [self.word2index[self.PADDING_TOKEN],\n",
    "                        self.word2index[self.END_TOKEN]\n",
    "                        ]\n",
    "        return ' '.join([self.index2word[idx] for idx in word_numbers if idx not in ignore_index])\n",
    "\n",
    "    @property\n",
    "    def start_token_index(self):\n",
    "        return self.word2index[self.START_TOKEN]\n",
    "\n",
    "\n",
    "class TrainingSession:\n",
    "    \"\"\"A container class that runs the training job\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, tokenizer, device, learning_rate, teacher_forcing_prob):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_prob = teacher_forcing_prob\n",
    "        self.start_token_index = self.tokenizer.start_token_index\n",
    "        self.pad_token_index = self.tokenizer.word2index[self.tokenizer.PADDING_TOKEN]\n",
    "        self.end_token_index = self.tokenizer.word2index[self.tokenizer.END_TOKEN]\n",
    "\n",
    "    def train(self, sources, targets, epochs=20):\n",
    "        encoder_optimizer = torch.optim.Adam(self.encoder.parameters(), lr=self.learning_rate)\n",
    "        decoder_optimizer = torch.optim.Adam(self.decoder.parameters(), lr=self.learning_rate)\n",
    "        batch_size = len(sources)\n",
    "        for epoch in range(epochs):\n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            encoder_hidden = (torch.zeros(1, batch_size, self.encoder.hidden_size).to(self.device),\n",
    "                              torch.zeros(1, batch_size, self.encoder.hidden_size).to(self.device))\n",
    "\n",
    "            encoder_input = torch.LongTensor(sources).to(self.device)\n",
    "            encoder_out, encoder_hidden = self.encoder(encoder_input, encoder_hidden)\n",
    "            loss = 0\n",
    "            for idx, (source, target) in enumerate(zip(sources, targets)):\n",
    "                # Extracting hidden states(h,c) for the current item in the batch\n",
    "                decoder_hidden = [encoder_hidden[0][:, idx:idx + 1].contiguous(),\n",
    "                                  encoder_hidden[1][:, idx: idx + 1].contiguous()]\n",
    "                decoder_input = torch.LongTensor([[self.start_token_index]]).to(self.device)\n",
    "                predicted_indexes = []\n",
    "                for target_idx in target:\n",
    "                    if target_idx == self.end_token_index:\n",
    "                        break\n",
    "                    decoder_out, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                    predicted_target = decoder_out.argmax(dim=2)\n",
    "                    actual_target = torch.LongTensor([[target_idx]]).to(self.device)\n",
    "                    # teacher forcing\n",
    "                    if np.random.random() < self.teacher_forcing_prob:\n",
    "                        decoder_input = actual_target\n",
    "                    else:\n",
    "                        decoder_input = predicted_target\n",
    "                    loss += F.cross_entropy(decoder_out.squeeze(0), actual_target.flatten(),\n",
    "                                            ignore_index=self.pad_token_index)\n",
    "                    predicted_indexes.append(predicted_target.item())\n",
    "                    if epoch % 5 == 0:\n",
    "                        # print(f'Epoch:{epoch}, Loss: {loss.item():.5f}')\n",
    "                        print(f'T{idx}', self.tokenizer.indexes_to_text(target))\n",
    "                        print(f'P{idx}:', self.tokenizer.indexes_to_text(predicted_indexes))\n",
    "                        print('----------------------------------')\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            print('epoc:', epoch, ' loss', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T0 i am good ,thank you.\n",
      "P0: morning!\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: morning! morning!\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: morning! morning! day?\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: morning! morning! day? was\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: morning! morning! day? was you.\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: morning!\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: morning! was\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: morning! was great\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: morning! was great fantastic\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: morning! was great fantastic morning!\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: morning!\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: morning! was\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: morning! was great\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: morning! was great great\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: morning! was great great you.\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: morning!\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: morning! your\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: morning! your you.\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: morning! your you. morning!\n",
      "----------------------------------\n",
      "epoc: 0  loss 64.2213134765625\n",
      "epoc: 1  loss 58.73884582519531\n",
      "epoc: 2  loss 53.10373306274414\n",
      "epoc: 3  loss 46.81049346923828\n",
      "epoc: 4  loss 42.38125228881836\n",
      "T0 i am good ,thank you.\n",
      "P0: good\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: good am\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: good am good\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: good am good ,thank\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: good am good ,thank you.\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: good\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: good was\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: good was a\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: good was a fantastic\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: good was a fantastic day\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: good\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: good good\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: good good a\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: good good a fantastic\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: good good a fantastic day\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: good\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: good good\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: good good to\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: good good to great\n",
      "----------------------------------\n",
      "epoc: 5  loss 36.13434982299805\n",
      "epoc: 6  loss 41.59748840332031\n",
      "epoc: 7  loss 34.15456008911133\n",
      "epoc: 8  loss 30.121410369873047\n",
      "epoc: 9  loss 28.787799835205078\n",
      "T0 i am good ,thank you.\n",
      "P0: things\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: things are\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: things are going\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: things are going great\n",
      "----------------------------------\n",
      "T0 i am good ,thank you.\n",
      "P0: things are going great you.\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: things\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: things are\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: things are going\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: things are going fantastic\n",
      "----------------------------------\n",
      "T1 it was a fantastic day\n",
      "P1: things are going fantastic day\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: things\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: things morning\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: things morning to\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: things morning to you\n",
      "----------------------------------\n",
      "T2 good morning to you too\n",
      "P2: things morning to you too\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: things\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: things are\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: things are going\n",
      "----------------------------------\n",
      "T3 things are going great\n",
      "P3: things are going great\n",
      "----------------------------------\n",
      "epoc: 10  loss 26.391098022460938\n",
      "epoc: 11  loss 17.063629150390625\n",
      "epoc: 12  loss 13.280055046081543\n",
      "epoc: 13  loss 11.348806381225586\n",
      "epoc: 14  loss 11.189927101135254\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(max_sequence_length=MAX_SEQUENCE_LENGTH)\n",
    "sources, targets = zip(*conversation_pair)\n",
    "tokenizer.fit_on_text(sources + targets)\n",
    "sources = tokenizer.texts_to_index(sources)\n",
    "targets = tokenizer.texts_to_index(targets)\n",
    "encoder = EncoderLSTM(input_size=tokenizer.words_count, hidden_size=LSTM_HIDDEN_SIZE,\n",
    "                      embeddings_dims=EMBEDDINGS_DIMS).to(\n",
    "    DEVICE)\n",
    "decoder = DecoderLSTM(input_size=tokenizer.words_count, hidden_size=LSTM_HIDDEN_SIZE, embeddings_dims=EMBEDDINGS_DIMS,\n",
    "                      vocab_size=tokenizer.words_count).to(DEVICE)\n",
    "trainer = TrainingSession(encoder=encoder, decoder=decoder, tokenizer=tokenizer, learning_rate=LEARNING_RATE,\n",
    "                          teacher_forcing_prob=TEACHER_FORCING_PROB,\n",
    "                          device=DEVICE)\n",
    "trainer.train(sources, targets, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
