{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Embedding, LSTM\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dummy sequence, You can assume these are indexes of our words.\n",
    "The objective of the whole process is to make a variable-length sequence suitable to be fed into an RNN(LSTM, GRU, etc) layer.\n",
    "Here are the steps:\n",
    "\n",
    "1. Create a variable-length sequence.\n",
    "2. Pad the sequences to make them of the same length\n",
    "3. Create an embedding for them.\n",
    "4. Pack the embeddings (to speedup the RNN calculations)\n",
    "5. Feed the (now packed) embeddings to LSTM to get outputs\n",
    "\n",
    "To achieve the goal, we are going to use two utility functions from PyTorch. \n",
    "\n",
    "- **pad_sequence** (Simply adds zeros to the sequences so that they all have the same size)\n",
    "- **pack_padded_sequence** ( Not necessarily required, but to be able to use the GPU more efficiently and speed up the RNN calculations) \n",
    "\n",
    "So the first methods pads (adds zeros) to the sequence, and the second method packs the previously padded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5], [6, 7, 8, 9, 10]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = [   \n",
    "    [1, 2, 3],\n",
    "    [4, 5],\n",
    "    [6, 7, 8, 9,10]\n",
    "]\n",
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting the padding thought, first we need to store the length of each sequence.\n",
    "We need these lengths so that later on, we know exactly how to pack them and get rid of extra zeros in each sequence.\n",
    "This way, we don't have to do additional calculations on some useless zeros(pad values) and this will speed up our RNN calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths=torch.LongTensor([len(sequence) for sequence in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeding_dim = 6\n",
    "#dictionary_size = 11\n",
    "\n",
    "embedding = Embedding(num_embeddings=11, embedding_dim=6)\n",
    "lstm = LSTM(input_size=6, hidden_size=2, batch_first=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  0,  0],\n",
       "        [ 4,  5,  0,  0,  0],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Padding\n",
    "sequences=[torch.LongTensor(sequence) for sequence in sequences]\n",
    "sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "sequences_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.1085,  0.4195, -0.1613, -1.6155, -0.6733, -0.0546],\n",
       "         [ 0.9498,  0.5771, -0.5823,  1.5888, -0.3406, -0.1107],\n",
       "         [ 1.6079,  0.1302, -0.8248,  2.2530, -0.5772,  0.5517],\n",
       "         [ 1.3814, -1.1445, -0.4823, -2.0504,  1.1077, -2.1600],\n",
       "         [ 1.3814, -1.1445, -0.4823, -2.0504,  1.1077, -2.1600]],\n",
       "\n",
       "        [[ 0.7458, -1.3027,  0.7759, -0.5413,  1.6839,  0.2756],\n",
       "         [ 1.9954,  0.1202, -0.6038,  1.1295, -0.1091,  1.0352],\n",
       "         [ 1.3814, -1.1445, -0.4823, -2.0504,  1.1077, -2.1600],\n",
       "         [ 1.3814, -1.1445, -0.4823, -2.0504,  1.1077, -2.1600],\n",
       "         [ 1.3814, -1.1445, -0.4823, -2.0504,  1.1077, -2.1600]],\n",
       "\n",
       "        [[ 1.6938, -1.3024,  0.5783,  0.0236, -0.6317, -0.6636],\n",
       "         [-0.0705,  1.1430, -1.3201,  1.0098, -0.5735, -0.3864],\n",
       "         [-0.4300, -0.7471,  0.0043, -0.6478, -3.1524,  1.5213],\n",
       "         [ 0.5949,  1.6353, -0.1353,  1.3254,  0.7167, -0.6114],\n",
       "         [ 0.1142, -0.4261,  0.2744, -0.8125,  1.1292,  0.0725]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Embedding\n",
    "sequences_embeded=embedding(sequences_padded)\n",
    "sequences_embeded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 1.6938, -1.3024,  0.5783,  0.0236, -0.6317, -0.6636],\n",
       "        [-2.1085,  0.4195, -0.1613, -1.6155, -0.6733, -0.0546],\n",
       "        [ 0.7458, -1.3027,  0.7759, -0.5413,  1.6839,  0.2756],\n",
       "        [-0.0705,  1.1430, -1.3201,  1.0098, -0.5735, -0.3864],\n",
       "        [ 0.9498,  0.5771, -0.5823,  1.5888, -0.3406, -0.1107],\n",
       "        [ 1.9954,  0.1202, -0.6038,  1.1295, -0.1091,  1.0352],\n",
       "        [-0.4300, -0.7471,  0.0043, -0.6478, -3.1524,  1.5213],\n",
       "        [ 1.6079,  0.1302, -0.8248,  2.2530, -0.5772,  0.5517],\n",
       "        [ 0.5949,  1.6353, -0.1353,  1.3254,  0.7167, -0.6114],\n",
       "        [ 0.1142, -0.4261,  0.2744, -0.8125,  1.1292,  0.0725]],\n",
       "       grad_fn=<PackPaddedSequenceBackward>), batch_sizes=tensor([3, 3, 2, 1, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Packing\n",
    "sequences_packed = pack_padded_sequence(sequences_embeded, sequence_lengths.cpu().numpy(), batch_first=True,enforce_sorted=False)\n",
    "sequences_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[ 0.0749, -0.1175],\n",
       "        [ 0.0207,  0.1377],\n",
       "        [ 0.1525,  0.0380],\n",
       "        [-0.0025, -0.1506],\n",
       "        [-0.0076, -0.0971],\n",
       "        [ 0.1263, -0.0889],\n",
       "        [ 0.0208, -0.5788],\n",
       "        [-0.0066, -0.1679],\n",
       "        [-0.0239, -0.0715],\n",
       "        [ 0.0845, -0.0212]], grad_fn=<CatBackward>), batch_sizes=tensor([3, 3, 2, 1, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LSTM\n",
    "outputput_packed, (hidden,context)=lstm(sequences_packed)\n",
    "outputput_packed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is LSTM, *output_packed* is now is a Named Tuple which provides some additional information that we might not care about. The actual output that we want is in *data* field since we might need to use it as in input to our LSTM in the next iteration.\n",
    "We can access the data (the actual output values of LSTM) in two ways\n",
    "\n",
    "**1.Easy way**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0749, -0.1175],\n",
       "        [ 0.0207,  0.1377],\n",
       "        [ 0.1525,  0.0380],\n",
       "        [-0.0025, -0.1506],\n",
       "        [-0.0076, -0.0971],\n",
       "        [ 0.1263, -0.0889],\n",
       "        [ 0.0208, -0.5788],\n",
       "        [-0.0066, -0.1679],\n",
       "        [-0.0239, -0.0715],\n",
       "        [ 0.0845, -0.0212]], grad_fn=<CatBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputput_packed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also need the *hidden state* of the last layer of LSTM (context vector).Possibly for your decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0066, -0.1679],\n",
       "         [ 0.1263, -0.0889],\n",
       "         [ 0.0845, -0.0212]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Slightly more involved way**\n",
    "\n",
    "*pad_packed_sequence*  might seems a bit confusing at the beginning but its role is actually very simple. Whenever we pack something we need to be able to unpack it again, right? (Think of zip and unzip). So here, this function just un-packs a sequence. (Which obviously should have already been packed). How does it do the unpacking? So, we just pass our packed sequence to the function to get its unpacked version back. It also returns the original length of each sequence for our convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0207,  0.1377],\n",
       "         [-0.0076, -0.0971],\n",
       "         [-0.0066, -0.1679],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.1525,  0.0380],\n",
       "         [ 0.1263, -0.0889],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.0749, -0.1175],\n",
       "         [-0.0025, -0.1506],\n",
       "         [ 0.0208, -0.5788],\n",
       "         [-0.0239, -0.0715],\n",
       "         [ 0.0845, -0.0212]]], grad_fn=<IndexSelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, input_sequence_sizes = pad_packed_sequence(outputput_packed, batch_first=True)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
